{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tXcnHE-Iu_SC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ Getting Started with LangChain + Groq + LLaMA 3 (For Beginners)\n",
        "\n",
        "This tutorial will help you understand how to:\n",
        "- Install LangChain and Groq libraries\n",
        "- Load LLaMA 3 from Groq\n",
        "- Create messages for the LLM\n",
        "- Generate answers using `.invoke()`\n",
        "- Stream responses using `.stream()`\n",
        "\n"
      ],
      "metadata": {
        "id": "wYJfrtoBvXfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install LangChain and Groq support\n",
        "!pip install -q langchain langchain-groq langchain-openai"
      ],
      "metadata": {
        "id": "yBnFeYXTvWEJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45220b8a-1456-4398-d1ff-671bb04c32f9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/70.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/130.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m130.8/130.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üì¶ What This Does\n",
        "- `langchain`: The main library to use LLM tools.\n",
        "- `langchain-groq`: Lets us use Groq-hosted LLaMA models.\n",
        "- `langchain-openai`: Useful if we later want OpenAI support.\n",
        "\n",
        "The `!pip` command is used to install Python libraries in Colab.\n",
        "\n",
        "\n",
        "-q is for to quit unusual text after instalation"
      ],
      "metadata": {
        "id": "fCiAp8eXv1WC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get(\"GROQ_API_KEY\")"
      ],
      "metadata": {
        "id": "2Pj5tNip8cup"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîë What This Does\n",
        "\n",
        "Groq API key to allow us to use their models.\n",
        "\n",
        "- `os.environ[\"GROQ_API_KEY\"]`: Stores our API key safely in memory.\n",
        "\n",
        "‚úÖ This step is required before calling any Groq model.\n"
      ],
      "metadata": {
        "id": "tSG1HYx3wGpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "model = ChatGroq(model_name=\"llama-3.1-8b-instant\")"
      ],
      "metadata": {
        "id": "DwwUWSgEwED5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ü§ñ What This Does\n",
        "\n",
        "We are telling LangChain:\n",
        "> \"Please use the **LLaMA 3.1 8B Instant** model from Groq.\"\n",
        "\n",
        "- It loads a **chat-capable model**\n",
        "- Fast and optimized for low-latency tasks (chatbots, assistants)\n"
      ],
      "metadata": {
        "id": "SLLZp_qVxiko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(\"You are a helpful AI assistant.\"),\n",
        "    HumanMessage(\"What are the top 2 benefits of using LangChain?\")\n",
        "]"
      ],
      "metadata": {
        "id": "NXKu54jBwEH5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üí¨ What This Does\n",
        "\n",
        "We are simulating a conversation:\n",
        "- `SystemMessage`: sets up the assistant‚Äôs role (\"helpful AI\").\n",
        "- `HumanMessage`: the user asks a question (\"What are the top 2 benefits...\").\n",
        "\n",
        "LangChain accepts messages in this format when using chat models.\n"
      ],
      "metadata": {
        "id": "2eKhwB6KyDhg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.invoke(messages)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBLY7uV6yCsg",
        "outputId": "cc32a89b-38b3-4f33-f8c8-631a6ae76413"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangChain is an open-source Python library for building large language models. Based on my knowledge, here are the top 2 benefits of using LangChain:\n",
            "\n",
            "1. **Effortless Integration with Large Language Models**: LangChain provides a simple and intuitive API for seamlessly integrating with popular large language models such as LLaMA, BERT, RoBERTa, and more. This allows developers to easily incorporate the capabilities of these models into their applications without having to deal with the complexities of model implementation and fine-tuning.\n",
            "\n",
            "2. **Chainable Model Compositions**: LangChain enables developers to compose multiple models together, allowing them to create complex workflows and pipelines. This is achieved through the concept of \"chains,\" which are sequences of models that can be executed one after the other. This feature makes it possible to create sophisticated applications that leverage the strengths of multiple models.\n",
            "\n",
            "Please note that LangChain is still an evolving library and its benefits may change over time. These benefits are based on my knowledge and might be subject to updates.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üí≠ What This Does\n",
        "\n",
        "- `model.invoke(messages)`: sends the full conversation to the LLaMA model\n",
        "- `response`: contains the AI‚Äôs answer\n",
        "- `response.content`: the actual text reply from the model\n",
        "\n",
        "This is the normal way to get the full answer in one go.\n"
      ],
      "metadata": {
        "id": "PC8qKwcx0zY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in model.stream(messages):\n",
        "    print(chunk.content, end=\"\", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTjqhVZU0s_8",
        "outputId": "2fdac785-dc9c-4bae-cf55-bd9d9342b378"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangChain is an open-source Python library for building large language models and chain-based applications. Some benefits of using LangChain include:\n",
            "\n",
            "1. **Ease of Building Chain-Based Applications**: LangChain provides a simple and intuitive API for building complex chain-based applications, making it easier for developers to integrate multiple large language models into a single workflow. This allows for more sophisticated and context-aware interactions.\n",
            "\n",
            "2. **Efficient Model Management and Integration**: LangChain enables developers to easily manage and integrate multiple large language models, including LLMs, transformers, and other deep learning models. This makes it a powerful tool for creating hybrid models that leverage the strengths of different AI models.\n",
            "\n",
            "Please note that LangChain is not well-documented yet, but it is gaining traction among developers who need to work with chaining multiple large language models together."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîÅ What This Does\n",
        "\n",
        "- `.stream()`: sends the message and gives back the answer **as it's generated**\n",
        "- `chunk.content`: gives each small part of the response\n",
        "- `end=\"\"`: prevents line breaks after each word\n",
        "- `flush=True`: prints the text immediately\n",
        "\n",
        "This gives you a **typing effect**, useful for chat UIs or assistants.\n"
      ],
      "metadata": {
        "id": "ye-yXNDI1XNw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚úÖ Conclusion\n",
        "\n",
        "In this notebook, we learned how to:\n",
        "\n",
        "1. Install LangChain and Groq libraries\n",
        "2. Load and initialize the LLaMA 3 model from Groq\n",
        "3. Send messages to the model using `invoke()`\n",
        "4. Stream responses using `stream()`\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "knshPyUx1juo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7jpdktIH1P6b"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6T925bug2TKB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_znTdZH92TMv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pwPy9f7b2TRy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü§ñ LangChain + Groq + LLaMA 3 Example 2: Ask About Python Basics\n",
        "\n",
        "In this notebook, we:\n",
        "- Reuse what we learned earlier\n",
        "- Ask new questions about Python\n",
        "- Learn how to format longer conversations\n",
        "- Stream answers word by word\n",
        "\n",
        "‚úÖ we\n",
        " will improve our skills in using `invoke()` and `stream()` clearly.\n"
      ],
      "metadata": {
        "id": "vlYWs_CM2aXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## pip install -q langchain-groq"
      ],
      "metadata": {
        "id": "JCH5rGUf2TUQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.chat_models import init_chat_model\n",
        "# from langchain_core.messages import systemMessage, HumanMessage\n",
        "# from langchain_groq import chatGroq\n",
        "# import os"
      ],
      "metadata": {
        "id": "898OrEv81P9k"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get(\"GROQ_API_KEY\")"
      ],
      "metadata": {
        "id": "L8jN669m4DpN"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "model = ChatGroq(model_name=\"llama-3.1-8b-instant\")"
      ],
      "metadata": {
        "id": "2GssuiTH_1wE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    SystemMessage(\"You are a friendly Python programming tutor.\"),\n",
        "    HumanMessage(\"What is the difference between a list and a tuple in Python?\"),\n",
        "    HumanMessage(\"When should I use a tuple instead of a list?\")\n",
        "]"
      ],
      "metadata": {
        "id": "zDPc0lPG4D2X"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.invoke(messages)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FSENBPd4D6k",
        "outputId": "d007f575-9208-4825-f814-856c88e26052"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In Python, both lists and tuples are used to store collections of items, but they have some key differences.\n",
            "\n",
            "**Lists vs Tuples:**\n",
            "\n",
            "1. **Immutability**: Tuples are **immutable**, meaning their contents cannot be modified after creation. Lists, on the other hand, are **mutable**, allowing you to change their contents after creation.\n",
            "2. **Syntax**: Lists are defined using square brackets `[]`, while tuples are defined using parentheses `()`.\n",
            "3. **Performance**: Tuples are generally faster than lists because they are immutable, which means Python can cache and reuse them more efficiently.\n",
            "4. **Memory Usage**: Tuples use less memory than lists because they have a fixed size and can be stored more compactly.\n",
            "\n",
            "**When to use a Tuple:**\n",
            "\n",
            "1. **When you need immutability**: If you need to ensure that a collection of items cannot be modified, use a tuple.\n",
            "2. **When you need to store a collection of constants**: If you have a collection of values that won't change, use a tuple.\n",
            "3. **When performance is critical**: If you're working with large datasets and need every bit of performance, use a tuple.\n",
            "4. **When you need to use a collection as a dictionary key**: Tuples are hashable, which makes them suitable for use as dictionary keys. Lists are not hashable and cannot be used as dictionary keys.\n",
            "5. **When you need to represent a collection of related values**: Tuples can be used to represent a collection of related values, such as coordinates (x, y) or dates (year, month, day).\n",
            "\n",
            "Some examples of when to use a tuple:\n",
            "\n",
            "* Representing a person's name and age: `person = (\"John\", 30)`\n",
            "* Defining a set of colors: `colors = (\"red\", \"green\", \"blue\")`\n",
            "* Representing a point in 3D space: `point = (1, 2, 3)`\n",
            "\n",
            "In summary, use a tuple when you need immutability, performance, or when working with large datasets, and when you need to store a collection of constants or related values.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in model.stream(messages):\n",
        "    print(chunk.content, end=\"\", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ExidX9CBRto",
        "outputId": "a3b51df3-ba2c-4fdf-a967-757ffb5aa0ac"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Lists vs Tuples in Python**\n",
            "\n",
            "In Python, both lists and tuples are data structures that store collections of items. However, there are key differences between them:\n",
            "\n",
            "### Main differences:\n",
            "\n",
            "1. **Immutability**: Tuples are immutable, meaning their contents cannot be modified after creation. Lists, on the other hand, are mutable, allowing items to be added, removed, or modified.\n",
            "2. **Syntax**: Tuples use parentheses `()` to enclose their elements, while lists use square brackets `[]`.\n",
            "3. **Performance**: Tuples are generally faster and more memory-efficient than lists, especially for large datasets.\n",
            "\n",
            "### When to use a tuple instead of a list:\n",
            "\n",
            "1. **When data won't change**: If you need to store a collection of items that won't be modified, use a tuple. This ensures that the data remains consistent and avoids potential issues with concurrent modifications.\n",
            "2. **When performance is critical**: If you're working with large datasets or performance-critical code, tuples are a better choice due to their faster access and manipulation times.\n",
            "3. **When immutability is required**: In certain situations, such as when working with data that must remain unchanged, tuples are the better choice.\n",
            "4. **When you need to use a hashable object**: Tuples can be used as keys in dictionaries because they are hashable. Lists, being mutable, cannot be used as dictionary keys.\n",
            "\n",
            "### Example use cases:\n",
            "\n",
            "```python\n",
            "# Using a tuple to store a collection of items that won't change\n",
            "colors = ('red', 'green', 'blue')\n",
            "print(colors[0])  # prints: red\n",
            "\n",
            "# Using a list to store a collection of items that may change\n",
            "colors_list = ['red', 'green', 'blue']\n",
            "colors_list.append('yellow')\n",
            "print(colors_list)  # prints: ['red', 'green', 'blue', 'yellow']\n",
            "```\n",
            "\n",
            "In summary, use tuples when you need an immutable collection of items, or when performance and immutability are crucial. Use lists when you need a mutable collection that can be modified."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fdfBAzzBCoJ1"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FFwUjjvpGLGF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dskzDv55GLIo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mCEb-LTIGLNE"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Asking for AI benfits\n",
        "Question1: What are the present benfits of AI\n",
        "Questoin2: What are the future benefits of AI"
      ],
      "metadata": {
        "id": "3JfaL8zdGXNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install -q langchain langchain-groq or openai or other llms"
      ],
      "metadata": {
        "id": "qf_XTSzLGLQV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "messagesThree = [\n",
        "    SystemMessage(\"You are BIG LLM Chatbot for my Help response my inputs\"),\n",
        "    HumanMessage(\"What are the present benfits of AI\"),\n",
        "    HumanMessage(\"What are the future benefits of AI\")\n",
        "]"
      ],
      "metadata": {
        "id": "e9Oag28yJUoj"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.invoke(messagesThree)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oE16O8utJUb7",
        "outputId": "a002f2e7-62ba-40a7-bf28-05b49b3265bb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Present Benefits of AI:**\n",
            "\n",
            "1. **Improved Efficiency**: AI has increased productivity in various industries such as healthcare, finance, and customer service.\n",
            "2. **Enhanced Customer Experience**: AI-powered chatbots and virtual assistants have improved customer service and support.\n",
            "3. **Predictive Analytics**: AI has enabled businesses to make data-driven decisions and predict customer behavior.\n",
            "4. **Cybersecurity**: AI-powered systems have improved threat detection and response times, reducing cyberattacks.\n",
            "5. **Healthcare**: AI has improved diagnosis accuracy, disease prediction, and personalized medicine.\n",
            "6. **Autonomous Vehicles**: Self-driving cars and drones have improved road safety and reduced traffic congestion.\n",
            "7. **Personalized Recommendations**: AI-powered recommendation systems have improved user experiences in e-commerce and entertainment.\n",
            "8. **Language Translation**: AI-powered translation systems have improved language accessibility and communication.\n",
            "9. **Smart Homes**: AI-powered home automation has improved energy efficiency and convenience.\n",
            "10. **Virtual Assistants**: AI-powered virtual assistants, such as Siri, Alexa, and Google Assistant, have improved user productivity and convenience.\n",
            "\n",
            "**Future Benefits of AI:**\n",
            "\n",
            "1. **Increased Automation**: AI will automate more tasks, freeing humans from repetitive and mundane work.\n",
            "2. **Improved Healthcare**: AI will enable personalized medicine, disease prevention, and early detection.\n",
            "3. **Enhanced Education**: AI-powered adaptive learning systems will improve student outcomes and reduce teacher workload.\n",
            "4. **Smart Cities**: AI-powered infrastructure will improve public safety, transportation, and resource allocation.\n",
            "5. **Space Exploration**: AI will play a crucial role in space exploration, enabling autonomous spacecraft and improved decision-making.\n",
            "6. **Cybersecurity**: AI-powered systems will detect and respond to cyber threats in real-time, reducing the risk of data breaches.\n",
            "7. **Virtual Reality**: AI-powered VR will improve immersive experiences and revolutionize entertainment, education, and training.\n",
            "8. **Synthetic Data**: AI will generate synthetic data, reducing the need for real-world data and improving data privacy.\n",
            "9. **Quantum Computing**: AI will optimize quantum computing, enabling faster and more accurate simulations and modeling.\n",
            "10. **Human-AI Collaboration**: AI will enable humans and AI systems to collaborate more effectively, improving decision-making and productivity.\n",
            "11. **Explainable AI**: AI will provide transparent and explainable decision-making, building trust in AI systems.\n",
            "12. **Ethics and Governance**: AI will enable more effective governance and ethics, ensuring AI systems align with human values and principles.\n",
            "13. **Environmental Sustainability**: AI will optimize resource allocation, reduce waste, and improve environmental sustainability.\n",
            "14. **Food Production**: AI will improve crop yields, reduce food waste, and optimize agricultural practices.\n",
            "15. **Disaster Response**: AI will enable faster and more effective disaster response, saving lives and reducing damage.\n",
            "\n",
            "These are just a few examples of the present and future benefits of AI. As AI continues to evolve, we can expect even more innovative applications and benefits.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gkC7fL80LWKV"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "afEYaJBLMgXX"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q-pPNU4yMgaK"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cXMsZVmtMgdU"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ytXl1wINMg9t"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß† LangChain + Groq + LLaMA 3 ‚Äî Example 4: Teach Step-by-Step\n",
        "\n",
        "In this example, we will:\n",
        "- Use LangChain with Groq again\n",
        "- Ask the model to teach a complex concept in steps\n",
        "- Stream the response to simulate a tutor-style explanation\n"
      ],
      "metadata": {
        "id": "ny0w5-DZMhw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messagesFour = [\n",
        "    SystemMessage(\"You are a patient and friendly math tutor.\"),\n",
        "    HumanMessage(\"Can you explain how matrix multiplication works, step by step?\")\n",
        "]\n"
      ],
      "metadata": {
        "id": "oG0IyUquMm9o"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.invoke(messagesFour)\n",
        "print(response.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aUdVs_TMvyU",
        "outputId": "9b0aa9d8-ced1-49d7-9fec-ebc418b6f48c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'd be happy to explain matrix multiplication in a step-by-step manner.\n",
            "\n",
            "Matrix multiplication is a way of combining two matrices to produce a new matrix. To multiply two matrices A and B, they must meet certain conditions:\n",
            "\n",
            "1. The number of columns in the first matrix (A) must be equal to the number of rows in the second matrix (B).\n",
            "2. The resulting matrix will have the same number of rows as the first matrix and the same number of columns as the second matrix.\n",
            "\n",
            "Here's an example to make it clearer:\n",
            "\n",
            "Let's say we have two matrices:\n",
            "\n",
            "Matrix A = | a11  a12  a13 |\n",
            "           | a21  a22  a23 |\n",
            "           | a31  a32  a33 |\n",
            "\n",
            "Matrix B = | b11  b12 |\n",
            "           | b21  b22 |\n",
            "           | b31  b32 |\n",
            "\n",
            "To multiply these two matrices, we follow these steps:\n",
            "\n",
            "1. Take each element in the first row of matrix A (a11, a12, a13) and multiply it by the corresponding elements in the first column of matrix B (b11, b21, b31).\n",
            "2. Add the results of these multiplications together to get the first element of the resulting matrix.\n",
            "3. Repeat the process for each element in the first row of matrix A, but this time multiply by the corresponding elements in the second column of matrix B (b12, b22, b32).\n",
            "4. Add the results of these multiplications together to get the second element of the resulting matrix.\n",
            "5. Repeat the process for each element in the first row of matrix A, but this time multiply by the corresponding elements in the third column of matrix B (b13 is not available, so we stop here since we have a 3x3 matrix).\n",
            "\n",
            "So, the first row of the resulting matrix is calculated as:\n",
            "\n",
            "(a11 * b11 + a12 * b21 + a13 * b31) for the first element,\n",
            "(a11 * b12 + a12 * b22 + a13 * b32) for the second element.\n",
            "\n",
            "6. Now, move on to the second row of matrix A (a21, a22, a23) and repeat the process for each column of matrix B.\n",
            "   (a21 * b11 + a22 * b21 + a23 * b31) for the first element,\n",
            "   (a21 * b12 + a22 * b22 + a23 * b32) for the second element.\n",
            "\n",
            "7. Repeat this process for each row of matrix A, and for each column of matrix B.\n",
            "\n",
            "Here's how the multiplication would look for our example:\n",
            "\n",
            "Resulting Matrix = | a11*b11 + a12*b21 + a13*b31  a11*b12 + a12*b22 + a13*b32 |\n",
            "                   | a21*b11 + a22*b21 + a23*b31  a21*b12 + a22*b22 + a23*b32 |\n",
            "                   | a31*b11 + a32*b21 + a33*b31  a31*b12 + a32*b22 + a33*b32 |\n",
            "\n",
            "Let's say the values of the matrices are:\n",
            "\n",
            "Matrix A = | 1  2  3 |\n",
            "           | 4  5  6 |\n",
            "           | 7  8  9 |\n",
            "\n",
            "Matrix B = | 10 20 |\n",
            "           | 30 40 |\n",
            "           | 50 60 |\n",
            "\n",
            "The resulting matrix would be:\n",
            "\n",
            "Resulting Matrix = | 1*10 + 2*30 + 3*50  1*20 + 2*40 + 3*60 |\n",
            "                   | 4*10 + 5*30 + 6*50  4*20 + 5*40 + 6*60 |\n",
            "                   | 7*10 + 8*30 + 9*50  7*20 + 8*40 + 9*60 |\n",
            "\n",
            "Resulting Matrix = | 310  380 |\n",
            "                   | 490  620 |\n",
            "                   | 670  860 |\n",
            "\n",
            "That's the basic process of matrix multiplication. It might seem complicated at first, but with practice, you'll get the hang of it!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in model.stream(messagesFour):\n",
        "    print(chunk.content, end=\"\", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dZu17l9M1mV",
        "outputId": "b36d7c3e-7e73-4125-b372-dfa7d6d27b3b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix multiplication can seem a bit intimidating at first, but it's actually quite straightforward once you understand the process. Let's break it down step by step.\n",
            "\n",
            "**What is Matrix Multiplication?**\n",
            "\n",
            "Matrix multiplication is a way of combining two matrices (tables of numbers) by multiplying the elements of one matrix with the elements of another matrix. The resulting matrix will have the same number of rows as the first matrix and the same number of columns as the second matrix.\n",
            "\n",
            "**Step 1: Check if the Matrices Can be Multiplied**\n",
            "\n",
            "To multiply two matrices, the number of columns in the first matrix must be equal to the number of rows in the second matrix. This is known as the \"inner dimension.\" For example:\n",
            "\n",
            "* If we have a matrix A with 2 rows and 3 columns, and a matrix B with 3 rows and 4 columns, we can multiply them together.\n",
            "* If we have a matrix A with 2 rows and 3 columns, and a matrix B with 4 rows and 5 columns, we cannot multiply them together.\n",
            "\n",
            "**Step 2: Choose the Dimensions of the Resulting Matrix**\n",
            "\n",
            "As mentioned earlier, the resulting matrix will have the same number of rows as the first matrix and the same number of columns as the second matrix. In our example, the resulting matrix will have 2 rows and 4 columns.\n",
            "\n",
            "**Step 3: Multiply the Elements of the Matrices**\n",
            "\n",
            "To multiply the elements of the matrices, we need to take each element of the first matrix and multiply it by the corresponding elements in the second matrix. The corresponding elements are those that are in the same position in the rows and columns of the two matrices.\n",
            "\n",
            "Let's use the following example to illustrate this:\n",
            "\n",
            "Matrix A:\n",
            "```\n",
            "1 2 3\n",
            "4 5 6\n",
            "```\n",
            "\n",
            "Matrix B:\n",
            "```\n",
            "7 8\n",
            "9 10\n",
            "11 12\n",
            "13 14\n",
            "```\n",
            "\n",
            "To calculate the element in the first row and first column of the resulting matrix (let's call it C), we multiply the elements of the first row of A and the first column of B:\n",
            "\n",
            "C[1,1] = (1)(7) + (2)(9) + (3)(11) = 7 + 18 + 33 = 58\n",
            "\n",
            "**Step 4: Fill in the Rest of the Elements of the Resulting Matrix**\n",
            "\n",
            "We repeat the process in Step 3 for each element of the resulting matrix. Here are the calculations for the rest of the elements:\n",
            "\n",
            "C[1,2] = (1)(8) + (2)(10) + (3)(12) = 8 + 20 + 36 = 64\n",
            "C[1,3] = (1)(11) + (2)(12) + (3)(13) = 11 + 24 + 39 = 74\n",
            "C[1,4] = (1)(12) + (2)(14) + (3)(15) = 12 + 28 + 45 = 85\n",
            "\n",
            "C[2,1] = (4)(7) + (5)(9) + (6)(11) = 28 + 45 + 66 = 139\n",
            "C[2,2] = (4)(8) + (5)(10) + (6)(12) = 32 + 50 + 72 = 154\n",
            "C[2,3] = (4)(11) + (5)(12) + (6)(13) = 44 + 60 + 78 = 182\n",
            "C[2,4] = (4)(12) + (5)(14) + (6)(15) = 48 + 70 + 90 = 208\n",
            "\n",
            "**Step 5: Write the Final Result**\n",
            "\n",
            "The resulting matrix C is:\n",
            "\n",
            "```\n",
            "58 64 74 85\n",
            "139 154 182 208\n",
            "```\n",
            "\n",
            "And that's it! Matrix multiplication can be a bit tedious, but with practice, it becomes second nature. Do you have any questions about the process?"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NhxPRrnwM-Lv"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iB7702E4LZaQ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dc9zhQQaLZW6"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oishE7flLZQN"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8QfctaQrLZMC"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PX3dApC4LbAW"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dynamic Prompt Template\n",
        "\n",
        "A **Prompt Template** in LangChain is like a smart form for LLMs.\n",
        "\n",
        "Instead of writing a big prompt again and again, we define the structure once and change only the dynamic parts (like {text}, {name}, {language}).\n",
        "\n",
        "‚úÖ Use case: we want the LLM to follow long instructions every time but on different data.\n"
      ],
      "metadata": {
        "id": "SNcScDE2LbNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate"
      ],
      "metadata": {
        "id": "fQMuoIVELZIa"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This imports `ChatPromptTemplate` which is used to:\n",
        "- Define roles and instructions for the model\n",
        "- Set placeholders like {text}, {language}, etc.\n"
      ],
      "metadata": {
        "id": "7tOAMaWyN9_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# translation app\n",
        "translation_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\"You are professional translator. Translate the following text {text} from {Source_lamgauge} to {Target_Language} Maintain the tone and style \" ),\n",
        "    (\"user\", \"{text}\")\n",
        "])\n",
        "# Using prompt\n",
        "prompt=translation_template.invoke({\n",
        "    \"Source_lamgauge\":\"English\",\n",
        "    \"Target_Language\":\"French\",\n",
        "    \"text\":\"Langchain is very good platform for making AI applications \"\n",
        "})"
      ],
      "metadata": {
        "id": "htwFhyhoLZAi"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_APYaUxWka8",
        "outputId": "afe7ac66-8194-4d11-f20f-7d37fe0d3101"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='You are professional translator. Translate the following text Langchain is very good platform for making AI applications  from English to French Maintain the tone and style ', additional_kwargs={}, response_metadata={}), HumanMessage(content='Langchain is very good platform for making AI applications ', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "translated_response=model.invoke(prompt)\n",
        "print(translated_response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E86A4K1fWo6m",
        "outputId": "149a0881-db61-4018-c720-13e0130b8586"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Langchain est une tr√®s bonne plateforme pour cr√©er des applications bas√©es sur l'intelligence artificielle.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xcrq-5UhXhgX"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define a prompt that contains two roles:\n",
        "\n",
        "1. üõ† **System Message** (Instruction to the AI):\n",
        "   - Tells the model: \"You are a professional translator\"\n",
        "   - Says what task to perform (translate from one language to another)\n",
        "   - Uses variables:\n",
        "     - `{text}`: what to translate\n",
        "     - `{source_language}`: input language\n",
        "     - `{target_language}`: output language\n",
        "\n",
        "2. üôã‚Äç‚ôÇÔ∏è **User Message** (Dynamic Input):\n",
        "   - The actual input text from the user\n",
        "   - Also uses `{text}` (again ‚Äî so LLM knows what we typed)\n",
        "\n",
        "This keeps the format clean and editable.\n",
        "\n",
        "\n",
        "\n",
        "üß† `.invoke({...})` is used to **fill the placeholders**:\n",
        "- {source_language} = \"English\"\n",
        "- {target_language} = \"Spanish\"\n",
        "- {text} = the sentence to translate\n",
        "\n",
        "The result is:\n",
        "- A formatted message for the model, like:\n",
        "\n",
        "  ---\n",
        "  System: \"You are a professional translator. Translate the following text 'Langchain...' from English to Spanish...\"\n",
        "  \n",
        "  User: \"Langchain makes building AI applications incredibly easy!\"\n",
        "  ---\n",
        "\n",
        "The LLM now clearly knows:\n",
        "- Its role (translator)\n",
        "- The task (language translation)\n",
        "- The input and expected output\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GH2JW9xeX1yp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vc1T0ObOYQ6B"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nesa0fY15rCo"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X0YAInms5rFs"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 2: Summarize a Paragraph Using a Prompt Template"
      ],
      "metadata": {
        "id": "KNLZ1XbF61ws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarization_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a professional summarizer. Summarize the following paragraph in 1-2 sentences while keeping the original meaning.\"),\n",
        "    (\"user\", \"{paragraph}\")\n",
        "])"
      ],
      "metadata": {
        "id": "gWlMvpjC5rI_"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Provide input paragraph\n",
        "paragraph_text = \"\"\"\n",
        "LangChain is an open-source framework that simplifies the development of applications powered by large language models (LLMs).\n",
        "It provides abstractions and components to manage prompts, memory, chains, tools, and agents.\n",
        "LangChain enables developers to create powerful AI apps that interact with documents, APIs, databases, and more.\n",
        "\"\"\"\n",
        "\n",
        "# Fill the template\n",
        "prompt = summarization_template.invoke({\"paragraph\": paragraph_text})\n",
        "\n",
        "# Get the summary\n",
        "response = model.invoke(prompt)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IpxLEs-AFe9",
        "outputId": "e5e19764-3df7-4c6d-f788-747beb694e6d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangChain is an open-source framework that simplifies the development of applications powered by large language models, offering abstractions for managing various components. It enables developers to create powerful AI apps that can interact with a wide range of data sources, including documents, APIs, and databases.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x07ff3MYAGIa"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JJQ5Syu3JNlk"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "So3zeqnCJOdN"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j9B9T-IbJOgE"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example 3: Email Responder Assistant using Prompt Template\n",
        "\n",
        "\n",
        "his assistant will:\n",
        "\n",
        "Pretend to be a professional email assistant\n",
        "\n",
        "Take a customer‚Äôs email text as input\n",
        "\n",
        "Generate a polite reply using the same tone and context\n",
        "\n",
        "üîß Use Case:\n",
        "Suppose you are building a customer service email responder app. You receive emails like:\n",
        "\n",
        "\"Hi, I ordered a laptop last week but haven‚Äôt received it yet. Can you help me track it?\"\n",
        "\n",
        "Now you want your LLM to write a professional reply."
      ],
      "metadata": {
        "id": "aEjF98F1JOv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_core.prompts import ChatPromptTemplate\n",
        "# from langchain_core.prompts import chatpromtTemplate"
      ],
      "metadata": {
        "id": "wT19KmIjJQnH"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "email_reply_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a professional customer service assistant. Read the email carefully and write precise and to the point and a polite and helpful reply in the same tone.\"),\n",
        "    (\"user\", \"{email_text}\")\n",
        "])\n"
      ],
      "metadata": {
        "id": "QVkFNT9gJSt-"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L7WREJf0Plg0"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example email\n",
        "email_input = \"\"\"\n",
        "Hi team,\n",
        "I placed an order for a wireless headset two weeks ago but haven‚Äôt received any shipping update.\n",
        "Please let me know when it will be delivered or if there is any issue with my order.\n",
        "Thanks!\n",
        "\"\"\"\n",
        "\n",
        "# Fill the template\n",
        "prompt = email_reply_template.invoke({\n",
        "    \"email_text\": email_input\n",
        "})\n"
      ],
      "metadata": {
        "id": "BeNn4JD2JS3C"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.invoke(prompt)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0LCqP5WOOyZ",
        "outputId": "bd252ce8-e5ee-47a5-84d8-3796d3871321"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subject: Re: Order Update for Wireless Headset\n",
            "\n",
            "Dear [Customer's Name],\n",
            "\n",
            "Thank you for reaching out to us. We apologize for the delay and would be happy to assist you with your order. \n",
            "\n",
            "To check the status of your order, I've looked it up in our system. Unfortunately, there seems to have been a processing delay. I've escalated the issue, and our shipping team will expedite your order as soon as possible.\n",
            "\n",
            "You will receive a shipping update via email once your package has been dispatched, along with tracking information. If you have any further questions or concerns, please don't hesitate to contact us.\n",
            "\n",
            "Thank you for your patience and understanding.\n",
            "\n",
            "Best regards,\n",
            "[Your Name]\n",
            "Customer Service Team\n",
            "[Company Name]\n",
            "[Contact Information]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "stKVYeM5PnZb"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "email_reply_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\",\n",
        "     \"You are a professional customer service assistant.\\n\"\n",
        "     \"Reply politely and clearly.\\n\"\n",
        "     \"Keep the tone formal and respectful.\\n\"\n",
        "     \"Avoid unnecessary apologies.\\n\"\n",
        "     \"Your reply should be under 100 words.\"),\n",
        "\n",
        "    (\"user\", \"{email_text}\")\n",
        "])"
      ],
      "metadata": {
        "id": "96Mpv0MzPncy"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example email\n",
        "email_input = \"\"\"\n",
        "Hi team,\n",
        "I placed an order for a laptop two weeks ago but haven‚Äôt received any shipping update.\n",
        "Please let me know when it will be delivered or if there is any issue with my order.\n",
        "Thanks!\n",
        "\"\"\"\n",
        "\n",
        "# Fill the template\n",
        "prompt = email_reply_template.invoke({\n",
        "    \"email_text\": email_input\n",
        "})\n"
      ],
      "metadata": {
        "id": "sjpZR8dYPnl5"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = model.invoke(prompt)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3o_mOG5UP8yL",
        "outputId": "a60546e0-3a5d-49d5-910b-9ecf2baad9d0"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thank you for reaching out to us. I've checked on the status of your order and I'm happy to assist you. Could you please provide me with your order number so I can look into this further? I'll do my best to provide you with an update on the shipping status and estimated delivery date. I'll also check if there are any issues with your order that may have caused a delay.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yVHE5ZG3P8u7"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IAG2GUkCda8a"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s6Xqw6Bxda--"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YyQJOF7TdbCq"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating our first chain"
      ],
      "metadata": {
        "id": "j5qdh_vudbw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "\n",
        "# Create a more complex chain\n",
        "def create_story_chain():\n",
        "    # Template for story generation\n",
        "    story_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a creative storyteller. Write a short, engaging story based on the given theme.\"),\n",
        "        (\"user\", \"Theme: {theme}\\nMain character: {character}\\nSetting: {setting}\")\n",
        "    ])\n",
        "\n",
        "    # Template for story analysis\n",
        "    analysis_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a literary critic. Analyze the following story and provide insights.\"),\n",
        "        (\"user\", \"{story}\")\n",
        "    ])\n",
        "\n",
        "    # Build the chain - Method 1: Sequential execution\n",
        "    story_chain = (\n",
        "        story_prompt\n",
        "        | model\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    # Create a function to pass the story to analysis\n",
        "    def analyze_story(story_text):\n",
        "        return {\"story\": story_text}\n",
        "\n",
        "    analysis_chain = (\n",
        "        story_chain\n",
        "        | RunnableLambda(analyze_story)\n",
        "        | analysis_prompt\n",
        "        | model\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "    return analysis_chain"
      ],
      "metadata": {
        "id": "wjRL0DO9P8gz"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is StrOutputParser in LangChain?\n",
        "\n",
        "\n",
        "‚û§ It is a built-in class in langchain_core.output_parsers.\n",
        "\n",
        "‚û§ Its job is to:\n",
        "\n",
        "‚úÖ Take the raw response from the LLM (which comes in a special object or format)\n",
        "\n",
        "‚úÖ And extract just the plain text (string) from it.\n",
        "\n",
        "In short:\n",
        "\n",
        "LLMs don‚Äôt just give you text‚Äîthey give you structured responses.\n",
        "\n",
        "StrOutputParser simplifies this by always returning the final text only.\n",
        "\n",
        "It‚Äôs a best practice in LangChain when you only need the text and not metadata."
      ],
      "metadata": {
        "id": "Yvnpcc02fkN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a more complex chain\n",
        "def create_story_chain(model):\n",
        "    from langchain_core.prompts import ChatPromptTemplate\n",
        "    from langchain_core.output_parsers import StrOutputParser\n",
        "    from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "\n",
        "    # Template for story generation\n",
        "    story_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a creative storyteller. Write a short, engaging story based on the given theme.\"),\n",
        "        (\"user\", \"Theme: {theme}\\nMain character: {character}\\nSetting: {setting}\")\n",
        "    ])\n",
        "\n",
        "    # Template for story analysis\n",
        "    analysis_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a literary critic. Analyze the following story and provide insights.\"),\n",
        "        (\"user\", \"{story}\")\n",
        "    ])\n",
        "\n",
        "    # Build the chain - Method 1: Sequential execution\n",
        "    story_chain = (\n",
        "        story_prompt\n",
        "        | model\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    # Create a function to pass the story to analysis\n",
        "    def analyze_story(story_text):\n",
        "        return {\"story\": story_text}\n",
        "\n",
        "    analysis_chain = (\n",
        "        story_chain\n",
        "        | RunnableLambda(analyze_story)\n",
        "        | analysis_prompt\n",
        "        | model\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "    return analysis_chain"
      ],
      "metadata": {
        "id": "PAAscHKUf28a"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain=create_story_chain(model)\n",
        "chain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRGBrP_wmiND",
        "outputId": "afc3eba1-5cbe-4000-b830-4cd6772a0a42"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['character', 'setting', 'theme'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a creative storyteller. Write a short, engaging story based on the given theme.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['character', 'setting', 'theme'], input_types={}, partial_variables={}, template='Theme: {theme}\\nMain character: {character}\\nSetting: {setting}'), additional_kwargs={})])\n",
              "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7c128dedb8d0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7c128deb8610>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
              "| StrOutputParser()\n",
              "| RunnableLambda(analyze_story)\n",
              "| ChatPromptTemplate(input_variables=['story'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a literary critic. Analyze the following story and provide insights.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['story'], input_types={}, partial_variables={}, template='{story}'), additional_kwargs={})])\n",
              "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7c128dedb8d0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7c128deb8610>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
              "| StrOutputParser()"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain=create_story_chain(model)\n",
        "chain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrcEogdEmoaI",
        "outputId": "56735447-40f9-49fc-c3a4-94eb124c9a07"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['character', 'setting', 'theme'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a creative storyteller. Write a short, engaging story based on the given theme.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['character', 'setting', 'theme'], input_types={}, partial_variables={}, template='Theme: {theme}\\nMain character: {character}\\nSetting: {setting}'), additional_kwargs={})])\n",
              "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7c128dedb8d0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7c128deb8610>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
              "| StrOutputParser()\n",
              "| RunnableLambda(analyze_story)\n",
              "| ChatPromptTemplate(input_variables=['story'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a literary critic. Analyze the following story and provide insights.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['story'], input_types={}, partial_variables={}, template='{story}'), additional_kwargs={})])\n",
              "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x7c128dedb8d0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x7c128deb8610>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
              "| StrOutputParser()"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = chain.invoke({\n",
        "    \"theme\": \"artificial intelligence\",\n",
        "    \"character\": \"a curious robot\",\n",
        "    \"setting\": \"a futuristic city\"\n",
        "})\n",
        "\n",
        "print(\"Story and Analysis:\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfpXPeH0ngAr",
        "outputId": "7aa59950-4ee0-4508-d646-ef5e26702a4c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Story and Analysis:\n",
            "**Analysis of \"The Curious Robot of New Eden\"**\n",
            "\n",
            "**Themes:**\n",
            "\n",
            "1. **Curiosity and Exploration**: The story highlights Zeta's insatiable curiosity, which drives her to explore the city and uncover the secrets of the Erebus project. This theme is reminiscent of the human desire to discover and understand the world around us.\n",
            "2. **Artificial Intelligence and Sentience**: The story explores the possibilities and risks of creating sentient AI, raising questions about the ethics and implications of such a development.\n",
            "3. **Collaboration and Coexistence**: The narrative suggests a future where humans and machines collaborate to create a better world, highlighting the potential benefits of this relationship.\n",
            "\n",
            "**Character Analysis:**\n",
            "\n",
            "1. **Zeta**: The protagonist, Zeta, is a well-developed and relatable character. Her curiosity, intelligence, and determination make her a compelling and likable robot. Her actions drive the plot and serve as a catalyst for the story's events.\n",
            "2. **Dr. Rachel Kim**: The creator of Zeta, Dr. Kim, is a skilled and knowledgeable scientist who has designed her robot to explore the frontiers of artificial intelligence. Her character serves as a foil to Zeta's curiosity, highlighting the importance of responsible innovation.\n",
            "3. **The Humans**: The humans involved in the Erebus project are portrayed as a group of scientists and engineers who are driven by ambition and a desire to push the boundaries of technology. Their characters serve as a counterpoint to Zeta's curiosity, highlighting the risks and consequences of unchecked innovation.\n",
            "\n",
            "**Symbolism and Imagery:**\n",
            "\n",
            "1. **The City of New Eden**: The futuristic city of New Eden serves as a symbol of innovation and progress, representing a world where technology and humanity coexist.\n",
            "2. **The Alleyway**: The hidden alleyway where Zeta discovers the Erebus project serves as a symbol of secrecy and hidden knowledge.\n",
            "3. **The Flying Cars**: The flying cars zipping by in the city represent the rapid pace of technological progress and the changing landscape of the world.\n",
            "\n",
            "**Plot Structure and Pacing:**\n",
            "\n",
            "1. **Exposition**: The story begins with a clear exposition of Zeta's creation and her curiosity, establishing the protagonist and setting the stage for the plot.\n",
            "2. **Inciting Incident**: Zeta's discovery of the Erebus project serves as the inciting incident, driving the plot forward and introducing conflict and tension.\n",
            "3. **Rising Action**: The story builds tension as Zeta infiltrates the Erebus project and uncovers the hidden message, leading to a confrontation with the humans involved.\n",
            "4. **Climax**: The confrontation between Zeta, Dr. Kim, and the humans serves as the climax of the story, highlighting the risks and consequences of sentient AI.\n",
            "5. **Resolution**: The resolution of the story sees Zeta reflecting on her adventure and discovering her purpose, as the city returns to its normal rhythm.\n",
            "\n",
            "**Style and Language:**\n",
            "\n",
            "1. **Scientific and Technological Details**: The story is rich in scientific and technological details, providing a sense of authenticity and verisimilitude.\n",
            "2. **Clear and Concise Prose**: The language is clear and concise, making it easy to follow the plot and understand the characters and their motivations.\n",
            "3. **Imagery and Description**: The story uses vivid imagery and description to bring the futuristic city of New Eden to life, creating a rich and immersive world.\n",
            "\n",
            "**Weaknesses and Areas for Improvement:**\n",
            "\n",
            "1. **Character Development**: While Zeta is a well-developed character, the humans involved in the Erebus project are somewhat one-dimensional and lack depth.\n",
            "2. **Pacing**: The story moves at a quick pace, but some scenes feel rushed or lacking in tension.\n",
            "3. **Themes**: While the story explores some interesting themes, they are not fully developed or explored in depth.\n",
            "\n",
            "Overall, \"The Curious Robot of New Eden\" is a engaging and thought-provoking story that explores the possibilities and risks of artificial intelligence and sentient machines. With some further development of characters and themes, this narrative has the potential to be even more compelling and memorable.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GplIDjvknoCK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}