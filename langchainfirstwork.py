# -*- coding: utf-8 -*-
"""LangChainFirstWork.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QQBZbfvdXDaKJrSe8eujkB5Z_jhhXtMR
"""



"""# üöÄ Getting Started with LangChain + Groq + LLaMA 3 (For Beginners)

This tutorial will help you understand how to:
- Install LangChain and Groq libraries
- Load LLaMA 3 from Groq
- Create messages for the LLM
- Generate answers using `.invoke()`
- Stream responses using `.stream()`


"""

# Install LangChain and Groq support
!pip install -q langchain langchain-groq langchain-openai

"""### üì¶ What This Does
- `langchain`: The main library to use LLM tools.
- `langchain-groq`: Lets us use Groq-hosted LLaMA models.
- `langchain-openai`: Useful if we later want OpenAI support.

The `!pip` command is used to install Python libraries in Colab.


-q is for to quit unusual text after instalation
"""

import os
from google.colab import userdata

os.environ["GROQ_API_KEY"] = userdata.get("GROQ_API_KEY")

"""### üîë What This Does

Groq API key to allow us to use their models.

- `os.environ["GROQ_API_KEY"]`: Stores our API key safely in memory.

‚úÖ This step is required before calling any Groq model.

"""

from langchain_groq import ChatGroq

model = ChatGroq(model_name="llama-3.1-8b-instant")

"""### ü§ñ What This Does

We are telling LangChain:
> "Please use the **LLaMA 3.1 8B Instant** model from Groq."

- It loads a **chat-capable model**
- Fast and optimized for low-latency tasks (chatbots, assistants)

"""

from langchain_core.messages import SystemMessage, HumanMessage

messages = [
    SystemMessage("You are a helpful AI assistant."),
    HumanMessage("What are the top 2 benefits of using LangChain?")
]

"""### üí¨ What This Does

We are simulating a conversation:
- `SystemMessage`: sets up the assistant‚Äôs role ("helpful AI").
- `HumanMessage`: the user asks a question ("What are the top 2 benefits...").

LangChain accepts messages in this format when using chat models.

"""

response = model.invoke(messages)
print(response.content)

"""### üí≠ What This Does

- `model.invoke(messages)`: sends the full conversation to the LLaMA model
- `response`: contains the AI‚Äôs answer
- `response.content`: the actual text reply from the model

This is the normal way to get the full answer in one go.

"""

for chunk in model.stream(messages):
    print(chunk.content, end="", flush=True)

"""### üîÅ What This Does

- `.stream()`: sends the message and gives back the answer **as it's generated**
- `chunk.content`: gives each small part of the response
- `end=""`: prevents line breaks after each word
- `flush=True`: prints the text immediately

This gives you a **typing effect**, useful for chat UIs or assistants.

# ‚úÖ Conclusion

In this notebook, we learned how to:

1. Install LangChain and Groq libraries
2. Load and initialize the LLaMA 3 model from Groq
3. Send messages to the model using `invoke()`
4. Stream responses using `stream()`

---
"""









"""# ü§ñ LangChain + Groq + LLaMA 3 Example 2: Ask About Python Basics

In this notebook, we:
- Reuse what we learned earlier
- Ask new questions about Python
- Learn how to format longer conversations
- Stream answers word by word

‚úÖ we
 will improve our skills in using `invoke()` and `stream()` clearly.

"""

## pip install -q langchain-groq

# from langchain.chat_models import init_chat_model
# from langchain_core.messages import systemMessage, HumanMessage
# from langchain_groq import chatGroq
# import os

import os
from google.colab import userdata

os.environ["GROQ_API_KEY"] = userdata.get("GROQ_API_KEY")

from langchain_groq import ChatGroq

model = ChatGroq(model_name="llama-3.1-8b-instant")

messages = [
    SystemMessage("You are a friendly Python programming tutor."),
    HumanMessage("What is the difference between a list and a tuple in Python?"),
    HumanMessage("When should I use a tuple instead of a list?")
]

response = model.invoke(messages)
print(response.content)

for chunk in model.stream(messages):
    print(chunk.content, end="", flush=True)









"""# Asking for AI benfits
Question1: What are the present benfits of AI
Questoin2: What are the future benefits of AI
"""

# pip install -q langchain langchain-groq or openai or other llms

from langchain_core.messages import SystemMessage, HumanMessage

messagesThree = [
    SystemMessage("You are BIG LLM Chatbot for my Help response my inputs"),
    HumanMessage("What are the present benfits of AI"),
    HumanMessage("What are the future benefits of AI")
]

response = model.invoke(messagesThree)
print(response.content)











"""# üß† LangChain + Groq + LLaMA 3 ‚Äî Example 4: Teach Step-by-Step

In this example, we will:
- Use LangChain with Groq again
- Ask the model to teach a complex concept in steps
- Stream the response to simulate a tutor-style explanation

"""

messagesFour = [
    SystemMessage("You are a patient and friendly math tutor."),
    HumanMessage("Can you explain how matrix multiplication works, step by step?")
]

response = model.invoke(messagesFour)
print(response.content)

for chunk in model.stream(messagesFour):
    print(chunk.content, end="", flush=True)













"""# Dynamic Prompt Template

A **Prompt Template** in LangChain is like a smart form for LLMs.

Instead of writing a big prompt again and again, we define the structure once and change only the dynamic parts (like {text}, {name}, {language}).

‚úÖ Use case: we want the LLM to follow long instructions every time but on different data.

"""

from langchain_core.prompts import ChatPromptTemplate

"""This imports `ChatPromptTemplate` which is used to:
- Define roles and instructions for the model
- Set placeholders like {text}, {language}, etc.

"""

# translation app
translation_template = ChatPromptTemplate.from_messages([
    ("system","You are professional translator. Translate the following text {text} from {Source_lamgauge} to {Target_Language} Maintain the tone and style " ),
    ("user", "{text}")
])
# Using prompt
prompt=translation_template.invoke({
    "Source_lamgauge":"English",
    "Target_Language":"French",
    "text":"Langchain is very good platform for making AI applications "
})

prompt

translated_response=model.invoke(prompt)
print(translated_response.content)



"""We define a prompt that contains two roles:

1. üõ† **System Message** (Instruction to the AI):
   - Tells the model: "You are a professional translator"
   - Says what task to perform (translate from one language to another)
   - Uses variables:
     - `{text}`: what to translate
     - `{source_language}`: input language
     - `{target_language}`: output language

2. üôã‚Äç‚ôÇÔ∏è **User Message** (Dynamic Input):
   - The actual input text from the user
   - Also uses `{text}` (again ‚Äî so LLM knows what we typed)

This keeps the format clean and editable.



üß† `.invoke({...})` is used to **fill the placeholders**:
- {source_language} = "English"
- {target_language} = "Spanish"
- {text} = the sentence to translate

The result is:
- A formatted message for the model, like:

  ---
  System: "You are a professional translator. Translate the following text 'Langchain...' from English to Spanish..."
  
  User: "Langchain makes building AI applications incredibly easy!"
  ---

The LLM now clearly knows:
- Its role (translator)
- The task (language translation)
- The input and expected output



"""







"""# Example 2: Summarize a Paragraph Using a Prompt Template"""

summarization_template = ChatPromptTemplate.from_messages([
    ("system", "You are a professional summarizer. Summarize the following paragraph in 1-2 sentences while keeping the original meaning."),
    ("user", "{paragraph}")
])

# Provide input paragraph
paragraph_text = """
LangChain is an open-source framework that simplifies the development of applications powered by large language models (LLMs).
It provides abstractions and components to manage prompts, memory, chains, tools, and agents.
LangChain enables developers to create powerful AI apps that interact with documents, APIs, databases, and more.
"""

# Fill the template
prompt = summarization_template.invoke({"paragraph": paragraph_text})

# Get the summary
response = model.invoke(prompt)
print(response.content)









"""# Example 3: Email Responder Assistant using Prompt Template


his assistant will:

Pretend to be a professional email assistant

Take a customer‚Äôs email text as input

Generate a polite reply using the same tone and context

üîß Use Case:
Suppose you are building a customer service email responder app. You receive emails like:

"Hi, I ordered a laptop last week but haven‚Äôt received it yet. Can you help me track it?"

Now you want your LLM to write a professional reply.
"""

# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.prompts import chatpromtTemplate

email_reply_template = ChatPromptTemplate.from_messages([
    ("system", "You are a professional customer service assistant. Read the email carefully and write precise and to the point and a polite and helpful reply in the same tone."),
    ("user", "{email_text}")
])



# Example email
email_input = """
Hi team,
I placed an order for a wireless headset two weeks ago but haven‚Äôt received any shipping update.
Please let me know when it will be delivered or if there is any issue with my order.
Thanks!
"""

# Fill the template
prompt = email_reply_template.invoke({
    "email_text": email_input
})

response = model.invoke(prompt)
print(response.content)



email_reply_template = ChatPromptTemplate.from_messages([
    ("system",
     "You are a professional customer service assistant.\n"
     "Reply politely and clearly.\n"
     "Keep the tone formal and respectful.\n"
     "Avoid unnecessary apologies.\n"
     "Your reply should be under 100 words."),

    ("user", "{email_text}")
])

# Example email
email_input = """
Hi team,
I placed an order for a laptop two weeks ago but haven‚Äôt received any shipping update.
Please let me know when it will be delivered or if there is any issue with my order.
Thanks!
"""

# Fill the template
prompt = email_reply_template.invoke({
    "email_text": email_input
})

response = model.invoke(prompt)
print(response.content)









"""## Creating our first chain"""

from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableLambda

# Create a more complex chain
def create_story_chain():
    # Template for story generation
    story_prompt = ChatPromptTemplate.from_messages([
        ("system", "You are a creative storyteller. Write a short, engaging story based on the given theme."),
        ("user", "Theme: {theme}\nMain character: {character}\nSetting: {setting}")
    ])

    # Template for story analysis
    analysis_prompt = ChatPromptTemplate.from_messages([
        ("system", "You are a literary critic. Analyze the following story and provide insights."),
        ("user", "{story}")
    ])

    # Build the chain - Method 1: Sequential execution
    story_chain = (
        story_prompt
        | model
        | StrOutputParser()
    )

    # Create a function to pass the story to analysis
    def analyze_story(story_text):
        return {"story": story_text}

    analysis_chain = (
        story_chain
        | RunnableLambda(analyze_story)
        | analysis_prompt
        | model
        | StrOutputParser()
    )
    return analysis_chain

"""## What is StrOutputParser in LangChain?


‚û§ It is a built-in class in langchain_core.output_parsers.

‚û§ Its job is to:

‚úÖ Take the raw response from the LLM (which comes in a special object or format)

‚úÖ And extract just the plain text (string) from it.

In short:

LLMs don‚Äôt just give you text‚Äîthey give you structured responses.

StrOutputParser simplifies this by always returning the final text only.

It‚Äôs a best practice in LangChain when you only need the text and not metadata.
"""

# Create a more complex chain
def create_story_chain(model):
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.output_parsers import StrOutputParser
    from langchain_core.runnables import RunnablePassthrough, RunnableLambda

    # Template for story generation
    story_prompt = ChatPromptTemplate.from_messages([
        ("system", "You are a creative storyteller. Write a short, engaging story based on the given theme."),
        ("user", "Theme: {theme}\nMain character: {character}\nSetting: {setting}")
    ])

    # Template for story analysis
    analysis_prompt = ChatPromptTemplate.from_messages([
        ("system", "You are a literary critic. Analyze the following story and provide insights."),
        ("user", "{story}")
    ])

    # Build the chain - Method 1: Sequential execution
    story_chain = (
        story_prompt
        | model
        | StrOutputParser()
    )

    # Create a function to pass the story to analysis
    def analyze_story(story_text):
        return {"story": story_text}

    analysis_chain = (
        story_chain
        | RunnableLambda(analyze_story)
        | analysis_prompt
        | model
        | StrOutputParser()
    )
    return analysis_chain

chain=create_story_chain(model)
chain

chain=create_story_chain(model)
chain

result = chain.invoke({
    "theme": "artificial intelligence",
    "character": "a curious robot",
    "setting": "a futuristic city"
})

print("Story and Analysis:")
print(result)

